import os
import json
import torch
import argparse
from tqdm import tqdm
from slugify import slugify
import multiprocessing as mp

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# ì„¤ì •
INDEX_DIR = "data/faiss_index"
EMBEDDING_MODEL_NAME = "Qwen/Qwen3-Embedding-0.6B"
BATCH_SIZE = 32  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥
MAX_WORKERS = min(4, mp.cpu_count())  # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ì¡°ì •
USE_CUDA = os.getenv("USE_CUDA", "false").lower() == "true"
FAISS_DISTANCE_STRATEGY = 'cosine'  # or 'euclidean'

def load_chunks(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f]

def build_documents(chunks: list[dict]) -> list[Document]:
    documents = []
    for i, chunk in enumerate(chunks):
        chunk_text = chunk.get("chunk_text", "").strip()
        if len(chunk_text) > 20:
            documents.append(Document(
                page_content=chunk_text,
                metadata={
                    "chunk_id": i,
                    "chunk_index": chunk.get("chunk_index", 0),
                    "title": chunk.get("title", ""),
                    "url": chunk.get("url", ""),
                    "source_type": chunk.get("source_type", "Unknown"),
                }
            ))
    return documents

def create_optimized_embeddings(documents, embedding_model, batch_size=BATCH_SIZE):
    """ë°°ì¹˜ ì²˜ë¦¬ë¡œ GPU ì‚¬ìš©ë¥ ì„ ìµœì í™”í•œ ì„ë² ë”© ìƒì„±"""
    print(f"ğŸš€ Creating embeddings with batch size: {batch_size}")
    
    # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    texts = [doc.page_content for doc in documents]
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    all_embeddings = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Processing batches"):
        batch_texts = texts[i:i + batch_size]
        batch_embeddings = embedding_model.embed_documents(batch_texts)
        all_embeddings.extend(batch_embeddings)
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    return all_embeddings

def main():
    p = argparse.ArgumentParser(description="Build FAISS index from text chunks")
    p.add_argument("--input", type=str, required=True,
                   help="Input JSONL file path containing text chunks")
    p.add_argument("--index_dir", type=str, default=INDEX_DIR,
                   help="Directory to save the FAISS index")
    p.add_argument("--embedding_model", type=str, default=EMBEDDING_MODEL_NAME,
                   help="HuggingFace model name for embeddings")
    p.add_argument("--batch_size", type=int, default=BATCH_SIZE,
                   help="Batch size for embedding generation")
    p.add_argument("--max_workers", type=int, default=MAX_WORKERS,
                   help="Number of parallel workers for processing")
    p.add_argument("--use_cuda", type=bool, default=USE_CUDA,
                   help="Use CUDA for GPU acceleration")
    p.add_argument("--faiss_distance_strategy", type=str, default=FAISS_DISTANCE_STRATEGY,
                   help="Distance strategy for FAISS (cosine or euclidean)")
    args = p.parse_args()
    
    print(f"ğŸ“¦ Loading chunks from: {args.input}")
    chunks = load_chunks(args.input)
    documents = build_documents(chunks)
    print(f"âœ… Loaded {len(documents)} documents")

    print(f"ğŸ¤– Loading embedding model: {args.embedding_model}")
    # GPU ì‚¬ìš© ì„¤ì • ë° ìµœì í™”
    device = 'cuda' if args.use_cuda else 'mps'
    print(f"ğŸ”§ Using device: {device}")
    
    # GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
    model_kwargs = {
        'device': device,
        'model_kwargs': {
            'torch_dtype': torch.float16 if device == 'cuda' else torch.float32,
        }
    }
    
    embedding_model = HuggingFaceEmbeddings(
        model_name=args.embedding_model, 
        model_kwargs=model_kwargs
    )

    print("ğŸ” Creating optimized FAISS index with batch processing...")
    
    # ìµœì í™”ëœ ë°©ì‹ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
    embeddings = create_optimized_embeddings(documents, embedding_model, args.batch_size)
    
    # FAISS ì¸ë±ìŠ¤ ìƒì„±
    print("ğŸ—ï¸ Building FAISS index...")
    vector_store = FAISS.from_embeddings(
        text_embeddings=list(zip([doc.page_content for doc in documents], embeddings)),
        embedding=embedding_model,
        metadatas=[doc.metadata for doc in documents],
        distance_strategy=args.faiss_distance_strategy
    )

    os.makedirs(args.index_dir, exist_ok=True)
    vector_store.save_local(args.index_dir)
    print(f"ğŸ’¾ Index saved to: {args.index_dir}/index.faiss and index.pkl")
    print(f"ğŸ¯ GPU optimization: Batch size {args.batch_size}, Workers {args.max_workers}")

if __name__ == "__main__":
    main()